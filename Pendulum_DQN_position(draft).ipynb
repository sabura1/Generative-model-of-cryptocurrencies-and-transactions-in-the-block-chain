{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p10Y890iw8LQ"
      },
      "source": [
        "# Задача о Маятнике\n",
        "\n",
        "Наша цель обучить несколько моделей для этой [задачи](https://gymnasium.farama.org/environments/classic_control/pendulum/):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdSrMos06uD_"
      },
      "source": [
        "## Подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04oKkVVYA9Ww"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1_pVLF_BFtG"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\", g=9.81)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyIosueRBHJg",
        "outputId": "1e1e8136-1c87-4bb8-cb4f-16e3fae4ef16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rRsL36zBH42",
        "outputId": "827dce3d-e430-46d5-d7ed-cc06e52b23b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0.4123625 ,  0.91101986, -0.89235795], dtype=float32), {})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "env.reset(seed=123, options={\"low\": -0.7, \"high\": 0.5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOXnNWBZE9rR",
        "outputId": "e1f83c55-d45c-4528-d0a6-b53b5d514ae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.1.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable_baselines3) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.6.0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable_baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable_baselines3-2.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install stable_baselines3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9IFr3oPwv0e"
      },
      "source": [
        "## Обучение с подкреплением на основе DQN (предсказываю положение через н секунд)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT92PAkrS3uz"
      },
      "source": [
        "### Обучение (n=5, дискретных действий = 21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BYdoxzZ3y359",
        "outputId": "e3e7d98d-adfb-467d-f5c1-e3a6f7563320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Используется устройство: cuda\n",
            "Эпизод 0/500, Средняя награда: -1368.69, Epsilon: 0.99\n",
            "Эпизод 10/500, Средняя награда: -1276.42, Epsilon: 0.95\n",
            "Эпизод 20/500, Средняя награда: -1342.92, Epsilon: 0.90\n",
            "Эпизод 30/500, Средняя награда: -1196.09, Epsilon: 0.86\n",
            "Эпизод 40/500, Средняя награда: -1342.37, Epsilon: 0.81\n",
            "Эпизод 50/500, Средняя награда: -1181.01, Epsilon: 0.77\n",
            "Эпизод 60/500, Средняя награда: -1410.05, Epsilon: 0.74\n",
            "Эпизод 70/500, Средняя награда: -1318.57, Epsilon: 0.70\n",
            "Эпизод 80/500, Средняя награда: -1158.15, Epsilon: 0.67\n",
            "Эпизод 90/500, Средняя награда: -941.05, Epsilon: 0.63\n",
            "Эпизод 100/500, Средняя награда: -913.89, Epsilon: 0.60\n",
            "Эпизод 110/500, Средняя награда: -940.77, Epsilon: 0.57\n",
            "Эпизод 120/500, Средняя награда: -755.72, Epsilon: 0.55\n",
            "Эпизод 130/500, Средняя награда: -686.66, Epsilon: 0.52\n",
            "Эпизод 140/500, Средняя награда: -652.03, Epsilon: 0.49\n",
            "Эпизод 150/500, Средняя награда: -533.90, Epsilon: 0.47\n",
            "Эпизод 160/500, Средняя награда: -512.78, Epsilon: 0.45\n",
            "Эпизод 170/500, Средняя награда: -502.66, Epsilon: 0.42\n",
            "Эпизод 180/500, Средняя награда: -549.35, Epsilon: 0.40\n",
            "Эпизод 190/500, Средняя награда: -541.13, Epsilon: 0.38\n",
            "Эпизод 200/500, Средняя награда: -443.61, Epsilon: 0.37\n",
            "Эпизод 210/500, Средняя награда: -420.64, Epsilon: 0.35\n",
            "Эпизод 220/500, Средняя награда: -375.81, Epsilon: 0.33\n",
            "Эпизод 230/500, Средняя награда: -469.21, Epsilon: 0.31\n",
            "Эпизод 240/500, Средняя награда: -461.95, Epsilon: 0.30\n",
            "Эпизод 250/500, Средняя награда: -450.71, Epsilon: 0.28\n",
            "Эпизод 260/500, Средняя награда: -536.24, Epsilon: 0.27\n",
            "Эпизод 270/500, Средняя награда: -487.55, Epsilon: 0.26\n",
            "Эпизод 280/500, Средняя награда: -513.30, Epsilon: 0.24\n",
            "Эпизод 290/500, Средняя награда: -461.16, Epsilon: 0.23\n",
            "Эпизод 300/500, Средняя награда: -480.87, Epsilon: 0.22\n",
            "Эпизод 310/500, Средняя награда: -437.44, Epsilon: 0.21\n",
            "Эпизод 320/500, Средняя награда: -458.80, Epsilon: 0.20\n",
            "Эпизод 330/500, Средняя награда: -432.63, Epsilon: 0.19\n",
            "Эпизод 340/500, Средняя награда: -410.16, Epsilon: 0.18\n",
            "Эпизод 350/500, Средняя награда: -390.81, Epsilon: 0.17\n",
            "Эпизод 360/500, Средняя награда: -376.66, Epsilon: 0.16\n",
            "Эпизод 370/500, Средняя награда: -429.66, Epsilon: 0.16\n",
            "Эпизод 380/500, Средняя награда: -463.07, Epsilon: 0.15\n",
            "Эпизод 390/500, Средняя награда: -446.00, Epsilon: 0.14\n",
            "Эпизод 400/500, Средняя награда: -374.86, Epsilon: 0.13\n",
            "Эпизод 410/500, Средняя награда: -414.91, Epsilon: 0.13\n",
            "Эпизод 420/500, Средняя награда: -374.46, Epsilon: 0.12\n",
            "Эпизод 430/500, Средняя награда: -476.14, Epsilon: 0.12\n",
            "Эпизод 440/500, Средняя награда: -337.98, Epsilon: 0.11\n",
            "Эпизод 450/500, Средняя награда: -432.09, Epsilon: 0.10\n",
            "Эпизод 460/500, Средняя награда: -430.19, Epsilon: 0.10\n",
            "Эпизод 470/500, Средняя награда: -461.40, Epsilon: 0.09\n",
            "Эпизод 480/500, Средняя награда: -422.44, Epsilon: 0.09\n",
            "Эпизод 490/500, Средняя награда: -330.61, Epsilon: 0.09\n",
            "\n",
            "Обучение вторичной сети...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Сбор данных: 100%|██████████| 200/200 [00:25<00:00,  7.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 0.02621\n",
            "Epoch 20: Loss = 0.00130\n",
            "Epoch 40: Loss = 0.00082\n",
            "Epoch 60: Loss = 0.00078\n",
            "Epoch 80: Loss = 0.00078\n",
            "Epoch 100: Loss = 0.00078\n",
            "Epoch 120: Loss = 0.00078\n",
            "Epoch 140: Loss = 0.00078\n",
            "Epoch 160: Loss = 0.00078\n",
            "Epoch 180: Loss = 0.00078\n",
            "Epoch 200: Loss = 0.00078\n",
            "Epoch 220: Loss = 0.00078\n",
            "Epoch 240: Loss = 0.00078\n",
            "Epoch 260: Loss = 0.00078\n",
            "Epoch 280: Loss = 0.00078\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL sklearn.preprocessing._data.StandardScaler was not an allowed global by default. Please use `torch.serialization.add_safe_globals([StandardScaler])` or the `torch.serialization.safe_globals([StandardScaler])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-73803f45f0ec>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nОбучение вторичной сети...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0mhidden_reps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_trajectory_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m \u001b[0msecondary_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_secondary_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_reps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;31m# Тестирование моделей\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-73803f45f0ec>\u001b[0m in \u001b[0;36mtrain_secondary_network\u001b[0;34m(hidden_reps, targets)\u001b[0m\n\u001b[1;32m    203\u001b[0m             }, 'best_secondary_net.pth')\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_secondary_net.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0msecondary_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0msecondary_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1468\u001b[0m                         )\n\u001b[1;32m   1469\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1470\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1471\u001b[0m                 return _load(\n\u001b[1;32m   1472\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL sklearn.preprocessing._data.StandardScaler was not an allowed global by default. Please use `torch.serialization.add_safe_globals([StandardScaler])` or the `torch.serialization.safe_globals([StandardScaler])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque, namedtuple\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Создаем среду\n",
        "env = gym.make('Pendulum-v1', render_mode=None)\n",
        "\n",
        "# Настройки\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.01\n",
        "EPS_DECAY = 0.995\n",
        "LR = 0.0005\n",
        "TARGET_UPDATE = 10\n",
        "MEMORY_SIZE = 20000\n",
        "HIDDEN_SIZE = 128\n",
        "N_FUTURE_STEPS = 5\n",
        "SEQUENCE_LENGTH = 10\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Используется устройство: {device}\")\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        hidden_representation = F.relu(self.fc2(x))\n",
        "        hidden_representation = self.dropout(hidden_representation)\n",
        "        return self.fc3(hidden_representation), hidden_representation\n",
        "\n",
        "class EnhancedSecondaryNetwork(nn.Module):\n",
        "    def __init__(self, hidden_size, state_size):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(hidden_size, 128, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.Linear(256, state_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        return self.fc(x[:, -1, :])\n",
        "\n",
        "def discretize_action(action_idx, action_size):\n",
        "    action_range = np.linspace(-2.0, 2.0, action_size)\n",
        "    return np.array([action_range[action_idx]])\n",
        "\n",
        "def select_action(state, policy_net, action_size, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return torch.tensor([[random.randrange(action_size)]], device=device, dtype=torch.long)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            q_values, _ = policy_net(state)\n",
        "            return q_values.max(1)[1].view(1, 1)\n",
        "\n",
        "def optimize_model(memory, policy_net, target_net, optimizer):\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return 0\n",
        "\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: not s, batch.done)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s, d in zip(batch.next_state, batch.done) if not d])\n",
        "\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    q_values, _ = policy_net(state_batch)\n",
        "    state_action_values = q_values.gather(1, action_batch)\n",
        "\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_q_values, _ = target_net(non_final_next_states)\n",
        "        next_state_values[non_final_mask] = next_q_values.max(1)[0]\n",
        "\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def collect_trajectory_data(policy_net, num_episodes=200):\n",
        "    states = []\n",
        "    hidden_reps = []\n",
        "    targets = []\n",
        "\n",
        "    policy_net.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(num_episodes), desc=\"Сбор данных\"):\n",
        "            state, _ = env.reset()\n",
        "            episode_states = []\n",
        "            episode_hidden = []\n",
        "\n",
        "            for _ in range(200):\n",
        "                state_tensor = torch.tensor(state, device=device, dtype=torch.float32).unsqueeze(0)\n",
        "                _, hidden = policy_net(state_tensor)\n",
        "\n",
        "                episode_states.append(state)\n",
        "                episode_hidden.append(hidden.cpu().numpy()[0])\n",
        "\n",
        "                action_idx = select_action(state_tensor, policy_net, action_size, epsilon=0.01)\n",
        "                action = discretize_action(action_idx.item(), action_size)\n",
        "                state, _, done, _, _ = env.step(action)\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            for i in range(len(episode_hidden) - SEQUENCE_LENGTH - N_FUTURE_STEPS):\n",
        "                hidden_seq = episode_hidden[i:i+SEQUENCE_LENGTH]\n",
        "                target_state = episode_states[i+SEQUENCE_LENGTH+N_FUTURE_STEPS]\n",
        "\n",
        "                hidden_reps.append(np.array(hidden_seq))\n",
        "                targets.append(target_state)\n",
        "\n",
        "    return np.array(hidden_reps), np.array(targets)\n",
        "\n",
        "def train_secondary_network(hidden_reps, targets):\n",
        "    scaler_x = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X = scaler_x.fit_transform(hidden_reps.reshape(-1, hidden_reps.shape[-1])).reshape(hidden_reps.shape)\n",
        "    y = scaler_y.fit_transform(targets)\n",
        "\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32, device=device)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.float32, device=device)\n",
        "\n",
        "    dataset = TensorDataset(X_tensor, y_tensor)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    secondary_net = EnhancedSecondaryNetwork(HIDDEN_SIZE, state_size).to(device)\n",
        "    optimizer = optim.AdamW(secondary_net.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
        "    criterion = nn.HuberLoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    secondary_net.train()\n",
        "\n",
        "    for epoch in range(100):\n",
        "        epoch_loss = 0\n",
        "        for batch_x, batch_y in loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            predictions = secondary_net(batch_x)\n",
        "            loss = criterion(predictions, batch_y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(secondary_net.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss /= len(loader)\n",
        "        scheduler.step(epoch_loss)\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(f'Epoch {epoch}: Loss = {epoch_loss:.5f}')\n",
        "\n",
        "        if epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            torch.save({\n",
        "                'model_state_dict': secondary_net.state_dict(),\n",
        "                'scaler_x': scaler_x,\n",
        "                'scaler_y': scaler_y\n",
        "            }, 'best_secondary_net.pth')\n",
        "\n",
        "    checkpoint = torch.load('best_secondary_net.pth')\n",
        "    secondary_net.load_state_dict(checkpoint['model_state_dict'])\n",
        "    secondary_net.eval()\n",
        "    return secondary_net, checkpoint['scaler_x'], checkpoint['scaler_y']\n",
        "\n",
        "class SecondaryPolicy:\n",
        "    def __init__(self, policy_net, secondary_net, scaler_x, scaler_y, horizon=5):\n",
        "        self.policy_net = policy_net\n",
        "        self.secondary_net = secondary_net\n",
        "        self.scaler_x = scaler_x\n",
        "        self.scaler_y = scaler_y\n",
        "        self.horizon = horizon\n",
        "        self.history = []\n",
        "\n",
        "        self.policy_net.eval()\n",
        "        self.secondary_net.eval()\n",
        "\n",
        "    def reset(self):\n",
        "        self.history = []\n",
        "\n",
        "    def act(self, state):\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.tensor(state, device=device, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "            _, hidden = self.policy_net(state_tensor)\n",
        "            hidden_np = hidden.cpu().numpy()[0]\n",
        "\n",
        "            self.history.append(hidden_np)\n",
        "            if len(self.history) > self.horizon:\n",
        "                self.history.pop(0)\n",
        "\n",
        "            if len(self.history) == self.horizon:\n",
        "                seq = np.array(self.history)\n",
        "                seq_scaled = self.scaler_x.transform(seq.reshape(-1, seq.shape[-1])).reshape(seq.shape)\n",
        "                seq_tensor = torch.tensor(seq_scaled, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "                predicted_scaled = self.secondary_net(seq_tensor)\n",
        "                predicted = self.scaler_y.inverse_transform(predicted_scaled.cpu().numpy())\n",
        "                predicted_tensor = torch.tensor(predicted, dtype=torch.float32, device=device)\n",
        "\n",
        "                q_values, _ = self.policy_net(predicted_tensor)\n",
        "                return q_values.argmax().item()\n",
        "\n",
        "            q_values, _ = self.policy_net(state_tensor)\n",
        "            return q_values.argmax().item()\n",
        "\n",
        "def test_model(model, num_episodes=50, is_secondary=False, policy_net=None, secondary_net=None, scaler_x=None, scaler_y=None):\n",
        "    rewards = []\n",
        "\n",
        "    if is_secondary:\n",
        "        policy = SecondaryPolicy(policy_net, secondary_net, scaler_x, scaler_y)\n",
        "\n",
        "    for _ in tqdm(range(num_episodes), desc=\"Тестирование\"):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        if is_secondary:\n",
        "            policy.reset()\n",
        "\n",
        "        for _ in range(200):\n",
        "            if is_secondary:\n",
        "                action_idx = policy.act(state)\n",
        "            else:\n",
        "                state_tensor = torch.tensor(state, device=device, dtype=torch.float32).unsqueeze(0)\n",
        "                action_idx = select_action(state_tensor, model, action_size, epsilon=0.01)\n",
        "\n",
        "            action = discretize_action(action_idx.item(), action_size)\n",
        "            state, reward, done, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# Основное обучение DQN\n",
        "state_size = 3\n",
        "action_size = 21\n",
        "num_episodes = 500\n",
        "\n",
        "policy_net = DQN(state_size, action_size, HIDDEN_SIZE).to(device)\n",
        "target_net = DQN(state_size, action_size, HIDDEN_SIZE).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
        "memory = ReplayMemory(MEMORY_SIZE)\n",
        "\n",
        "episode_rewards = []\n",
        "losses = []\n",
        "epsilon = EPS_START\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    state = torch.tensor([state], device=device, dtype=torch.float32)\n",
        "    total_reward = 0\n",
        "    episode_loss = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    truncated = False\n",
        "\n",
        "    while not (done or truncated):\n",
        "        action_idx = select_action(state, policy_net, action_size, epsilon)\n",
        "        action = discretize_action(action_idx.item(), action_size)\n",
        "        next_state, reward, done, truncated, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        reward = torch.tensor([reward], device=device, dtype=torch.float32)\n",
        "        next_state = torch.tensor([next_state], device=device, dtype=torch.float32)\n",
        "        done_tensor = torch.tensor([done or truncated], device=device, dtype=torch.bool)\n",
        "\n",
        "        memory.push(state, action_idx, next_state, reward, done_tensor)\n",
        "        state = next_state\n",
        "\n",
        "        loss = optimize_model(memory, policy_net, target_net, optimizer)\n",
        "        if loss > 0:\n",
        "            episode_loss += loss\n",
        "            steps += 1\n",
        "\n",
        "    if episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    epsilon = max(EPS_END, epsilon * EPS_DECAY)\n",
        "    episode_rewards.append(total_reward)\n",
        "    losses.append(episode_loss / steps if steps > 0 else 0)\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "        avg_reward = np.mean(episode_rewards[-10:])\n",
        "        print(f\"Эпизод {episode}/{num_episodes}, Средняя награда: {avg_reward:.2f}, Epsilon: {epsilon:.2f}\")\n",
        "# Обучение вторичной сети\n",
        "print(\"\\nОбучение вторичной сети...\")\n",
        "hidden_reps, targets = collect_trajectory_data(policy_net)\n",
        "secondary_net, scaler_x, scaler_y = train_secondary_network(hidden_reps, targets)\n",
        "def train_secondary_network(hidden_reps, targets):\n",
        "    scaler_x = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X = scaler_x.fit_transform(hidden_reps.reshape(-1, hidden_reps.shape[-1])).reshape(hidden_reps.shape)\n",
        "    y = scaler_y.fit_transform(targets)\n",
        "\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32, device=device)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.float32, device=device)\n",
        "\n",
        "    dataset = TensorDataset(X_tensor, y_tensor)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    secondary_net = EnhancedSecondaryNetwork(HIDDEN_SIZE, state_size).to(device)\n",
        "    optimizer = optim.AdamW(secondary_net.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
        "    criterion = nn.HuberLoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    best_net = None\n",
        "    best_scaler_x = None\n",
        "    best_scaler_y = None\n",
        "    secondary_net.train()\n",
        "\n",
        "    for epoch in range(100):\n",
        "        epoch_loss = 0\n",
        "        for batch_x, batch_y in loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            predictions = secondary_net(batch_x)\n",
        "            loss = criterion(predictions, batch_y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(secondary_net.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss /= len(loader)\n",
        "        scheduler.step(epoch_loss)\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(f'Epoch {epoch}: Loss = {epoch_loss:.5f}')\n",
        "\n",
        "        if epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            # Сохраняем копии лучших объектов\n",
        "            best_net = type(secondary_net)(HIDDEN_SIZE, state_size).to(device)\n",
        "            best_net.load_state_dict(secondary_net.state_dict())\n",
        "            best_scaler_x = type(scaler_x)().fit(hidden_reps.reshape(-1, hidden_reps.shape[-1]))\n",
        "            best_scaler_y = type(scaler_y)().fit(targets)\n",
        "\n",
        "    best_net.eval()\n",
        "    return best_net, best_scaler_x, best_scaler_y\n",
        "# Тестирование моделей\n",
        "print(\"\\nТестирование DQN...\")\n",
        "dqn_rewards = test_model(policy_net)\n",
        "\n",
        "print(\"\\nТестирование вторичной сети...\")\n",
        "secondary_rewards = test_model(\n",
        "    secondary_net,\n",
        "    is_secondary=True,\n",
        "    policy_net=policy_net,\n",
        "    secondary_net=secondary_net,\n",
        "    scaler_x=scaler_x,\n",
        "    scaler_y=scaler_y\n",
        ")\n",
        "\n",
        "# Визуализация\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(dqn_rewards, label='DQN')\n",
        "plt.plot(secondary_rewards, label='Вторичная сеть')\n",
        "plt.title('Награды по эпизодам')\n",
        "plt.xlabel('Эпизод')\n",
        "plt.ylabel('Награда')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.boxplot([dqn_rewards, secondary_rewards], labels=['DQN', 'Вторичная сеть'])\n",
        "plt.title('Распределение наград')\n",
        "plt.ylabel('Награда')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn4HwpXoS94E"
      },
      "source": [
        "### Метрики"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVMq5z8TTVK9"
      },
      "outputs": [],
      "source": [
        "# Вместо текущих графиков обучения DQN\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# График наград с усреднением\n",
        "plt.subplot(1, 2, 1)\n",
        "window_size = max(1, len(episode_rewards)//10)\n",
        "smoothed_rewards = np.convolve(episode_rewards, np.ones(window_size)/window_size, mode='valid')\n",
        "plt.plot(episode_rewards, alpha=0.3, label='Сырые награды')\n",
        "plt.plot(smoothed_rewards, color='red', linewidth=2, label=f'Скользящее среднее (окно={window_size})')\n",
        "plt.title('Динамика наград DQN')\n",
        "plt.xlabel('Эпизод')\n",
        "plt.ylabel('Награда')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# График потерь с логарифмической шкалой\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(losses, alpha=0.8)\n",
        "plt.yscale('log')  # Логарифмическая шкала для лучшей читаемости\n",
        "plt.title('Потери DQN (log scale)')\n",
        "plt.xlabel('Эпизод')\n",
        "plt.ylabel('Потеря')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('improved_dqn_training.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GQxqEdvCTi2"
      },
      "outputs": [],
      "source": [
        "def test_model(model, env, num_episodes=50, is_secondary=False):\n",
        "    rewards = []\n",
        "    policy_net.eval()  # Устанавливаем режим оценки\n",
        "\n",
        "    if is_secondary:\n",
        "        secondary_net.eval()\n",
        "\n",
        "    with torch.no_grad():  # Отключаем вычисление градиентов\n",
        "        for _ in range(num_episodes):\n",
        "            state, _ = env.reset()\n",
        "            state_tensor = torch.tensor(state, device=device, dtype=torch.float32)\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "            truncated = False\n",
        "            steps = 0\n",
        "\n",
        "            while not (done or truncated):\n",
        "                if not is_secondary:\n",
        "                    # Обычная DQN политика\n",
        "                    q_values, _ = model(state_tensor.unsqueeze(0))\n",
        "                    action_idx = q_values.max(1)[1]\n",
        "                else:\n",
        "                    # Используем вторичную сеть для предсказания состояния\n",
        "                    _, hidden_rep = policy_net(state_tensor.unsqueeze(0))\n",
        "                    predicted_state = secondary_net(hidden_rep)\n",
        "                    # Используем предсказанное состояние для выбора действия\n",
        "                    q_values, _ = policy_net(predicted_state)\n",
        "                    action_idx = q_values.max(1)[1]\n",
        "\n",
        "                action = discretize_action(action_idx.item(), action_size)\n",
        "                next_state, reward, done, truncated, _ = env.step(action)\n",
        "                total_reward += reward\n",
        "                state = next_state\n",
        "                state_tensor = torch.tensor(state, device=device, dtype=torch.float32)\n",
        "                steps += 1\n",
        "\n",
        "                # Ранняя остановка если застряли\n",
        "                if steps > 500:  # Максимальная длина эпизода\n",
        "                    break\n",
        "\n",
        "            rewards.append(total_reward)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# Ускоренное тестирование с прогресс-баром\n",
        "from tqdm import tqdm\n",
        "\n",
        "def fast_test_model(model, num_episodes=50, is_secondary=False):\n",
        "    rewards = []\n",
        "    policy_net.eval()\n",
        "    if is_secondary:\n",
        "        secondary_net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(num_episodes), desc=\"Тестирование\"):\n",
        "            state, _ = env.reset()\n",
        "            state_tensor = torch.tensor(state, device=device, dtype=torch.float32)\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "\n",
        "            # Используем numpy для быстрых вычислений на CPU\n",
        "            state_np = state\n",
        "            for _ in range(200):  # Максимум 200 шагов\n",
        "                if not is_secondary:\n",
        "                    q_values, _ = model(state_tensor.unsqueeze(0))\n",
        "                    action_idx = q_values.cpu().numpy().argmax()\n",
        "                else:\n",
        "                    _, hidden_rep = policy_net(state_tensor.unsqueeze(0))\n",
        "                    predicted_state = secondary_net(hidden_rep)\n",
        "                    q_values, _ = policy_net(predicted_state)\n",
        "                    action_idx = q_values.cpu().numpy().argmax()\n",
        "\n",
        "                action = discretize_action(action_idx, action_size)\n",
        "                next_state, reward, done, _, _ = env.step(action)\n",
        "                total_reward += reward\n",
        "                state_np = next_state\n",
        "                state_tensor = torch.tensor(state_np, device=device, dtype=torch.float32)\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            rewards.append(total_reward)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# Сравнение моделей\n",
        "print(\"Тестирование DQN модели...\")\n",
        "dqn_rewards = fast_test_model(policy_net, num_episodes=50, is_secondary=False)\n",
        "\n",
        "print(\"\\nТестирование модели с вторичной сетью...\")\n",
        "secondary_rewards = fast_test_model(policy_net, num_episodes=50, is_secondary=True)\n",
        "\n",
        "# Визуализация результатов\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# График распределения наград\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.violinplot(data=[dqn_rewards, secondary_rewards])\n",
        "plt.xticks([0, 1], ['DQN', 'Вторичная сеть'])\n",
        "plt.title('Распределение наград')\n",
        "plt.ylabel('Награда за эпизод')\n",
        "\n",
        "# График скользящего среднего\n",
        "plt.subplot(1, 2, 2)\n",
        "window_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Убедимся, что модели в режиме оценки\n",
        "policy_net.eval()\n",
        "secondary_net.eval()\n",
        "\n",
        "# Тестирование только вторичной сети с детальной диагностикой\n",
        "def analyze_secondary_network(num_episodes=20):\n",
        "    all_rewards = []\n",
        "    state_errors = []\n",
        "    action_discrepancies = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for ep in range(num_episodes):\n",
        "            state, _ = env.reset()\n",
        "            state_tensor = torch.tensor(state, device=device, dtype=torch.float32)\n",
        "            episode_rewards = 0\n",
        "            episode_errors = []\n",
        "            episode_discrepancies = []\n",
        "\n",
        "            for step in range(200):  # Максимальная длина эпизода\n",
        "                # Получаем скрытое представление и предсказание\n",
        "                _, hidden_rep = policy_net(state_tensor.unsqueeze(0))\n",
        "                predicted_state_scaled = secondary_net(hidden_rep)\n",
        "                predicted_state = scaler_y.inverse_transform(predicted_state_scaled.cpu().numpy())\n",
        "                predicted_state_tensor = torch.tensor(predicted_state, device=device, dtype=torch.float32)\n",
        "\n",
        "                # Ошибка предсказания состояния\n",
        "                current_error = mean_squared_error(state, predicted_state[0])\n",
        "                episode_errors.append(current_error)\n",
        "\n",
        "                # Разница в действиях между основной и вторичной сетью\n",
        "                q_main, _ = policy_net(state_tensor.unsqueeze(0))\n",
        "                action_main = q_main.argmax().item()\n",
        "\n",
        "                q_secondary, _ = policy_net(predicted_state_tensor)\n",
        "                action_secondary = q_secondary.argmax().item()\n",
        "                episode_discrepancies.append(1 if action_main != action_secondary else 0)\n",
        "\n",
        "                # Применяем действие от вторичной сети\n",
        "                action = discretize_action(action_secondary, action_size)\n",
        "                next_state, reward, done, _, _ = env.step(action)\n",
        "                episode_rewards += reward\n",
        "\n",
        "                state = next_state\n",
        "                state_tensor = torch.tensor(state, device=device, dtype=torch.float32)\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            all_rewards.append(episode_rewards)\n",
        "            state_errors.append(np.mean(episode_errors))\n",
        "            action_discrepancies.append(np.mean(episode_discrepancies))\n",
        "\n",
        "            print(f\"Эпизод {ep+1}: Награда={episode_rewards:.1f}, \"\n",
        "                  f\"Средняя ошибка состояния={np.mean(episode_errors):.4f}, \"\n",
        "                  f\"Несовпадение действий={np.mean(episode_discrepancies)*100:.1f}%\")\n",
        "\n",
        "    return all_rewards, state_errors, action_discrepancies\n",
        "\n",
        "# Запускаем анализ\n",
        "secondary_rewards, state_errors, action_diffs = analyze_secondary_network(num_episodes=20)\n",
        "\n",
        "# Визуализация проблем\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# График наград\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(secondary_rewards, 'o-')\n",
        "plt.title('Награды вторичной сети')\n",
        "plt.xlabel('Эпизод')\n",
        "plt.ylabel('Награда')\n",
        "plt.grid(True)\n",
        "\n",
        "# График ошибок предсказания состояния\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(state_errors, 'o-', color='orange')\n",
        "plt.title('Ошибка предсказания состояния')\n",
        "plt.xlabel('Эпизод')\n",
        "plt.ylabel('MSE')\n",
        "plt.grid(True)\n",
        "\n",
        "# График расхождений в действиях\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.bar(range(len(action_diffs)), action_diffs, color='red')\n",
        "plt.title('Расхождение действий с основной сетью')\n",
        "plt.xlabel('Эпизод')\n",
        "plt.ylabel('% несовпадений')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Сравнение с основной сетью\n",
        "print(\"\\nСравнение средних показателей:\")\n",
        "print(f\"Средняя награда DQN: {np.mean(dqn_rewards):.1f}\")\n",
        "print(f\"Средняя награда вторичной сети: {np.mean(secondary_rewards):.1f}\")\n",
        "print(f\"Средняя ошибка предсказания: {np.mean(state_errors):.4f}\")\n",
        "print(f\"Среднее расхождение действий: {np.mean(action_diffs)*100:.1f}%\")"
      ],
      "metadata": {
        "id": "axphLjeEwFD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4CYUPunIwyqr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "YdSrMos06uD_"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
